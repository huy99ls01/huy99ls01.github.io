---
title: "Recurrent Neural Networks"
subtitle: "Recurrent Neural Networks hoạt động như nào?"
layout: post
tags: [Recurrent Neural Networks, RNN]
---

## Sequence Data 

 RNN là mạng neural rất tốt trong việc mô hình hóa dữ liệu. Để hiểu được nó trước tiên hãy làm 1 thí nghiệm. Giả sử bạn có ảnh(still snapshot) của một quả bóng đang di chuyển. 
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/ball.png" alt="None">
 </center>
 
 Từ hình ảnh bên trên chúng ta muốn dự đoán hướng đi của quả bóng. Vậy chỉ với thông tin được cho bên trên, làm sao có thể dự đoán hướng đi của quả bóng? well, chúng ta có thể đoán hướng đi của quả bóng, nhưng đó sẽ là câu câu trả lời ngẫu nhiên phải không nào :)). Khi không có thông tin quả bóng đã từng ở đâu trước đó, thì ta không có đủ dữ liệu để có thể dự đoán được hướng đi của quả bóng.
 
 Còn nếu ta chụp liên tiếp vị trí của quả bóng, và có rất nhiều tấm hình cho thấy vị trí trước đó của quả bóng. Chúng ta sẽ sẽ có đủ thông tin để dự đoán hướng đi của quả bóng.

 <center>
  <img src="/img/bp/2018-06-18-RNN/ball.gif" alt="None">
 </center>

Vậy đây gọi là sequence data(chuỗi dữ liệu trình tự) nó dược tạo bởi một thứ tự cụ thể trong đó một vật theo sau bởi vật khác. Với thông tin này giờ chúng ta đã có thể dự đoán được quả bóng đang di chuyển sang bên phải

Sequence data(chuỗi dữ liệu trình tự) có nhiều dạng khác nhau. Audio(âm thanh) là 1 dạng chuỗi dữ liệu tự nhiên. Chúng ta có thể cắt các phổ(spectrogram) âm thanh thành 1 khối và đưa vào mạng RNN.

 <center>
  <img src="/img/bp/2018-06-18-RNN/audio.png" alt="None">
  <br>
  <i>Audio spectrogram chopped into chunks</i>
 </center>

Văn bản cũng là 1 dạng khác của chuỗi dữ liệu. Được chia nhỏ thành chuỗi các chữ cái hoặc chuỗi các từ(word).

## Sequential Memory

Ok, vậy thì RNN rất tốt trong việc xử lý chuỗi để dự đoán. But How?

Well, họ làm điều đó bằng cách có 1 khái niệm được gọi là sequential memory(bộ nhớ trình tự). Để hiểu được sequential memory là gì trước tiên hãy nhớ lại bảng chữ cái và đọc lại nó.

 <center>
  <img src="/img/bp/2018-06-18-RNN/abc.png" alt="None">
 </center>
 
 Khá là dễ phải không. Bởi vì đó là do chúng ta đã được dạy đọc bảng chữ cái theo một thứ tự nhất định, vậy nên chúng ta sẽ nhanh chóng đọc được nó một cách dễ dàng.
 
 Bây giờ thử đọc bảng chữ cái theo chiều ngược lại.
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/abcre.png" alt="None">
 </center>
 
 Trừ khi chúng ta được tập luyện đọc ngược bảng chữ cái theo chiều ngược lại thì sẽ thấy dễ hơn, còn nếu không thì sẽ rất khó khăn để đọc nó.
 
 Hoặc là bạn có thể đọc từ chữ cái F.
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/fabc.png" alt="None">
 </center>
 
 Lúc đầu bạn sẽ thấy khó khăn để đọc vài chữ cái đầu tiên, nhưng sau khi não của bạn nhớ ra thứ tự/trình tự của bảng chữ cái thì phần còn lại sẽ rất dễ đọc. 
 
Khi học bảng chữ cái chúng ta thường học theo chuỗi thứ tự từ A-Z, sequential memory giúp não của chúng ta dễ dàng hơn trong việc nhận dạng bảng chữ cái và các mô hình khác theo chuỗi thứ tự/trình tự.

## Recurrent Neural Network

Vậy RNN(Recurrent Neural Network) có một khái niệm trừu tượng về sequential memory(bộ nhớ trình tự), nhưng làm thế nào RNN có thể tái tạo lại nó từ khái niệm ấy. Hãy nhìn xuống hình bên dưới là mạng neural truyền thống và cũng được biết đến là feed-forward neural network. Một mạng neural sẽ có ba phần cơ bản là input layer, hidden layer và output layer.

 <center>
  <img src="/img/bp/2018-06-18-RNN/FFW.png" alt="None">
 <br>
 <em>Feed Forward Neural Network</em>
 </center>

Để tạo nên mạng recurrent câu hỏi đặt ra là, làm sao để có một feed-forward neural network mà có thể dùng thông tin trước đó để ảnh hưởng tới thông tin đằng sau?. Nếu chúng ta thêm vào một vòng lặp trong neural network mà có thể đưa thông tin trước đó lên phía trước?

<center>
  <img src="/img/bp/2018-06-18-RNN/RNN.png" alt="None">
 <br>
 <em>Recurrent Neural Network</em>
 </center>

Recurrent neural network có một cơ chế vồng lặp đóng vai trò như một đường cao tốc cho phép thông tin đi từ bước này đến bước tiếp theo. Về cơ bản đó là quá trình thực hiện của mạng recurrent

<center>
  <img src="/img/bp/2018-06-18-RNN/hidden.gif" alt="None">
 <br>
 <em>Passing Hidden State to next time step</em>
 </center>

 RNN sẽ đưa các hidden state tới các node hiện tại, hidden state đại diện cho các input trước đó. Hãy cùng xem qua một ứng dụng sử dụng RNN để hiểu rõ hơn về cách nó hoạt động.

Giả sử chúng ta muốn xây dựng một con chatbot, và con chatbot này có thể phân loại được mục đích của người dùng từ văn bản người dùng nhập vào.

<center>
  <img src="/img/bp/2018-06-18-RNN/classify.gif" alt="None">
 <br>
 <em>Classifying intents from users inputs</em>
 </center>
 
 Để giải quyết vấn đề này. Đầu tiên chúng ta sẽ sử dụng RNN để encode sequence of text(chuỗi văn bản). Sau đó chúng ta sẽ đưa output của RNN vào feed-forward neural network và nó sẽ phân loại mục đích của người viết.
 
 Khi mà user gõ từ bàn phím một câu hỏi "What time is it?" Đầu tiên chúng ta sẽ chia chuỗi văn bản thành các từ riêng biệt. Vì mạng recurrent làm việc theo trình tự nên chúng ta sẽ đưa vào rnn từng từ một.
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/breakingsen.gif" alt="None">
 <br>
 <em>Breaking up a sentence into word sequences</em>
 </center>
 
 Bước đầu tiên sẽ đưa từ "What" vào mạng RNN. RNN sẽ encode từ "what" và đưa ra một output.
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/breakingsen2.gif" alt="None">
 </center>
 
 Tiếp theo đó là từ "time" sẽ được đưa vào cùng với nó là hidden state từ bước trước(output của từ what trước đó). Và hiện giờ RNN sẽ có thông tin của 2 từ là "What" và "time".
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/breakingsen3.gif" alt="None">
 </center>
 
 Chúng ta tiếp tục lặp lại các bước như vậy cho đến khi encode hết tất các thông tin của các từ trong chuỗi từ các bước trước đó.
 
  <center>
  <img src="/img/bp/2018-06-18-RNN/breakingsen4.gif" alt="None">
 </center>
 
 Khi mà final output được tạo bởi tất cả các từ trong một chuỗi. Và giờ chúng ta có thể lấy final output để đưa vào feed-forward layer và phân loại mục đích của chuỗi văn bản đó.
 
  <center>
  <img src="/img/bp/2018-06-18-RNN/breakingsen5.gif" alt="None">
 </center>
 
## Vanishing Gradient
 
 Chúng ta có thể thấy hình tròn tượng trưng cho hidden state bên dưới có sự phân bố màu sắc không đều. Hình tròn này sẽ minh mọa cho một vấn đề trong mạng RNN là short-term memory.
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/final.png" alt="None">
 <br>
 <em>Final hidden state of the RNN</em>
 </center>

Nguyên nhân dẫn đến short-term memory là một vấn đề được nhắc đến rất nhiều trong neural network và các mạng neural khác, đó là Vanishing Gradient. RNN sẽ gặp vấn đền trong việc lưu trữ thông tin của các bước khi mà nó chạy càng nhiều bước. Hình hình tròn bên trên cho thấy phân bố màu từ rất ít cho đến nhiều, mỗi màu tượng trưng cho các từ theo thứ tự, "What", "Time", "is", "it" và "?", từ đó cho thấy "What" và "Time" có rất ít thông tin và gân như là không tồn tại, khi mà mạng RNN xử lý đến bước cuối cùng. Short-term memory và vanishing gradient xảy ra là do bản chất của back-propagation. Back-prop là một thuật toán dùng để train và tối ưu mạng neural. Trước tiên cùng xem lại sự tác động của back-pro đến deep feed-forward network.

Trainning neural network có 3 bước. Đầu tiên là đưa dữ liệu vào, foward và thực hiện dự đoán. Sau đó so sanh kết quả vừa dự đoán được với giá trị thật(ground-truth) bằng cách sử dụng hàm mất mát(loss function). Đầu ra của loss function là một giá trị sai số dùng để đánh giá xem mạng neural có hoat động tốt hay không, giá trị sai số này càng cao thì độ chính xác càng kém và ngược lại. Cuối cùng back-prop dùng nó để tính toán gradient cho mỗi node trong neural-network. 

<center>
  <img src="/img/bp/2018-06-18-RNN/backpropa.gif" alt="None">
 </center>
 
 Giá trị của gradient dùng để điều chỉnh các trọng số(weights) trong mạng neural giúp cho nó có thể học được từ dữ liệu. Gradient càng lớn nghĩa là phải điều chỉnh các trọng số càng nhiều và ngược lại. Khi thực hiện back-prop mỗi node trong layer tính toán gradient của nó dựa trên tác động của gradient trong layer trước đố. Vậy nên nếu sự điều chỉnh trọng số trong layer trước đó nhỏ thì các bước sau sẽ càng nhỏ hơn.
 
 Điều đó dẫn đến giá trị gradient bé dần theo cấp số mũ mỗi khi thực hiện một bước back-prop. Và từ đó khiến các layer trước đó sẽ không học được gì khi mà trọng số của nó gần như không được điều chỉnh vì giá trị gradient của nó cực kì bé. Và đó vấn đề của neural network gọi là vanishing gradient.

<center>
  <img src="/img/bp/2018-06-18-RNN/backpropashrink.gif" alt="None">
 <br>
 <em>Gradients shrink as it back-propagates down</em>
 </center>
 
 Để train RNN, chúng ta sử dụng một ứng dụng của back-propagation gọi là back-propagation through time. Khi thực hiện back-prop through time, cứ qua mỗi bước(time step) giá trị của gradient giảm(shrink) dần theo cấp số mũ, mỗi time step của RNN được coi như là một layer trong neural network.
 
<center>
  <img src="/img/bp/2018-06-18-RNN/backpropatime.gif" alt="None">
 <br>
 <em>Gradients shrink as it back-propagates through time</em>
 </center>
 
 Gradient dùng để điều chỉnh trọng số(weights) của mạng neural để cho nó có thể học được từ dữ liệu. Gradient bé sẽ dẫn đến việc các layer trước đó sẽ không thể học được gì.
 
 RNN sẽ không thể học được từ những thông tin quá xa trước đó do vanishing gradient. Và khả năng cao là các từ "What" và "time" sẽ không được cân nhắc khi dự đoán mục đích của người dùng. RNN sẽ sử dụng "is it?" để dự đoán nhiều hơn là "What time", điều đó khiến việc dự đoán thực sự rất mơ hồ khó khăn ngay cả với con người. Vậy từ việc không thể học được từ những thông tin trước đó dẫn đến việc RNN có một vấn đề là short-term memory.
 
 
 Để giải quyết vấn đề này mạng RNN đã được upgrade, LSTM(long short-term memory) và  GRU(gated recurrent units), RNN sẽ sử dụng một trong hai mô hình LSTM hoặc GRU.

 Bài gốc bằng tiếng ảnh ở [đây](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9).
