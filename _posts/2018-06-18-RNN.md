---
title: "Recurrent Neural Networks"
subtitle: "Recurrent Neural Networks hoạt động như nào?"
layout: post
tags: [Recurrent Neural Networks, RNN]
---

## Sequence Data 

 RNN là mạng neural rất tốt trong việc mô hình hóa dữ liệu. Để hiểu được nó trước tiên hãy làm 1 thí nghiệm. Giả sử bạn có ảnh(still snapshot) của một quả bóng đang di chuyển. 
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/ball.png" alt="None">
 </center>
 
 Giả sử như chúng ta muốn dự đoán hướng đi của quả bóng. Vậy chỉ với thông tin được cho bên trên, làm sao có thể dự đoán được? well,có thể đoán hướng đi của quả bóng, nhưng đó sẽ là câu câu trả lời ngẫu nhiên :)). Khi chúng không có thông tin quả bóng đã từng ở đâu, thì sẽ không có đủ dữ liệu để dự đoán được hướng đi của quả bóng.
 
 Nếu chụp liên tiếp vị trí của quả bóng. Chúng ta sẽ sẽ có đủ thông tin để dự đoán hướng đi của quả bóng.

 <center>
  <img src="/img/bp/2018-06-18-RNN/ball.gif" alt="None">
 </center>

Vậy đây là một chuỗi theo một thứ tự cụ thể trong đó một vật theo sau bởi vật khác. Với thông tin này giờ chúng ta đã có thể dự đoán được quả bóng đang di chuyển sang bên phải

Sequence data(dữ liệu trình tự) có nhiều dạng khác nhau. Audio(âm thanh) là 1 dạng sequence tự nhiên. Chúng ta có thể cắt các phổ âm thanh thành 1 khối và đưa vào mạng RNN.

 <center>
  <img src="/img/bp/2018-06-18-RNN/audio.png" alt="None">
  <br>
  <em>Phổ âm thanh được cắt ra thành nhiều khối</em>
 </center>

văn bản cũng là 1 dạng khác của chuỗi(sequence). Được chia nhỏ thành chuỗi các chữ cái hoặc chuỗi các từ(word).

## Sequential Memory

Ok, vậy thì RNN rất tốt trong việc xử lý chuỗi để dự đoán. But How?

Well, họ làm điều đó bằng cách có 1 khái niệm được gọi là Sequential Memory(bộ nhớ trình tự). Để hiểu được Sequential Memory, hãy nhớ lại bảng chữ cái và đọc lại nó.

 <center>
  <img src="/img/bp/2018-06-18-RNN/abc.png" alt="None">
 </center>
 
 Khá là dễ đúng không :)). Bởi vì chúng ta đã được dạy đọc trình tự bảng chữ cái này từ nhỏ, nên chúng ta sẽ nhanh chóng đọc được nó.
 
 Bây giờ thử đọc bảng chữ cái theo chiều ngược lại.
 

 <center>
  <img src="/img/bp/2018-06-18-RNN/abcre.png" alt="None">
 </center>
 
 chắc chắn là khó hơn :)), Trừ khi bạn được tập luyện để đọc ngược bảng chữ cái không thì sẽ rất khó khăn để đọc nó.
 
 Hoặc là bạn có thể đọc từ chữ cái F.
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/fabc.png" alt="None">
 </center>
 
 Lúc đầu bạn sẽ vật lộn với vài chữ cái đầu tiên, nhưng sau khi não của bạn nhớ ra trình tự của bảng chữ cái thì phần còn lại sẽ rất dễ đọc. 
 
Khi học bảng chữ cái chúng ta thường học theo chuỗi trình tự. Sequential Memory giúp não của chúng ta dễ dàng hơn trong việc nhận dạng các mô hình theo chuỗi trình tự.

## Recurrent Neural Network

Vậy RNN có khái niệm trừu tượng về Sequential Memory(bộ nhớ trình tự), nhưng làm thế nào RNN có thể tái tạo lại nó từ khái niệm đó. Hãy nhìn xuống hình bên dưới là mạng neural truyền thống và cũng được biết đến là feed-forward neural network. Neural Network có input layer, hidden layer và output layer.

 <center>
  <img src="/img/bp/2018-06-18-RNN/FFW.png" alt="None">
 <br>
 <em>Feed Forward Neural Network</em>
 </center>

Làm sao để có một feed-forward neural network mà có thể dùng thông tin trước đó để ảnh hưởng tới thông tin đằng sau?. Nếu chúng ta thêm vào một vòng lặp trong neural network mà có thể đưa thông tin trước đó lên phía trước?

<center>
  <img src="/img/bp/2018-06-18-RNN/RNN.png" alt="None">
 <br>
 <em>Recurrent Neural Network</em>
 </center>

Về cơ bản đó là những gì mà recurrent neural network làm. RNN(Recurrent neural network) có một cơ chế vồng lặp đóng vai trò như một đường cao tốc cho phép thông tin đi từ bước này đến bước tiếp theo.

<center>
  <img src="/img/bp/2018-06-18-RNN/hidden.gif" alt="None">
 <br>
 <em>Passing Hidden State to next time step</em>
 </center>

Thông tin này là hidden state, là đại diện cho các input trước đó. Hãy cùng xem qua một ứng dụng sử dụng RNN để hiểu rõ hơn về cách nó hoạt động.

Giả sử chúng ta muốn xây dựng một con chatbot, và con chatbot này có thể phân loại được mục đích của người dùng từ văn bản người dùng nhập vào.

<center>
  <img src="/img/bp/2018-06-18-RNN/classify.gif" alt="None">
 <br>
 <em>Classifying intents from users inputs</em>
 </center>
 
 Để giải quyết vấn đề này. Đầu tiên chúng ta sẽ sử dụng RNN để encode sequence of text(chuỗi văn bản). Sau đó chúng ta sẽ đưa output của RNN vào feed-forward neural network và nó sẽ phân loại mục đích của người viết.
 
 Khi mà user gõ từ bàn phím What time is it? Đầu tiên chúng ta sẽ chia chuỗi văn bản thành các từ riêng biệt. Vì mạng recurrent làm việc theo trình tự nên chúng ta sẽ đưa vào từng từ một.
 
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/breakingsen.gif" alt="None">
 <br>
 <em>Breaking up a sentence into word sequences</em>
 </center>
 
 Bước đầu tiên sẽ đưa từ "What" vào mạng RNN. RNN sẽ encode từ "what" và đưa ra kết quả là output.
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/breakingsen2.gif" alt="None">
 </center>
 
 Tiếp theo đó là từ "time" sẽ được đưa vào cùng với nó là hidden state từ bước trước. Và hiện giờ RNN sẽ có thông tin của 2 từ là "What" và "time".
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/breakingsen3.gif" alt="None">
 </center>
 
 Chúng ta tiếp tục lặp lại các bước như vậy cho đến khi encode hết tất các thông tin của các từ trong chuỗi từ các bước trước đó.
 
  <center>
  <img src="/img/bp/2018-06-18-RNN/breakingsen4.gif" alt="None">
 </center>
 
 Khi mà final output được tạo bởi tất cả các từ trong một chuỗi. Và giờ chúng ta có thể lấy final output để đưa vào feed-forward layer và phân loại mục đích của chuỗi văn bản đó.
 
  <center>
  <img src="/img/bp/2018-06-18-RNN/breakingsen5.gif" alt="None">
 </center>
 
## Vanishing Gradient
 
 Chúng ta có thể thấy hình tròn tượng trưng cho hidden state bên dưới có sự phân bố màu sắc không đều. Hình tròn này sẽ làm hình minh mọa cho một vấn đề trong mạng RNN là short-term memory.
 
 <center>
  <img src="/img/bp/2018-06-18-RNN/final.png" alt="None">
 <br>
 <em>Final hidden state of the RNN</em>
 </center>

Nguyên nhân dẫn đến Short-term memory là một vấn đề được nhắc đến rất nhiều trong neural network và các mạng neural khác, đó là Vanishing Gradient. RNN sẽ gặp vấn đền trong việc lưu trữ thông tin của các bước khi mà nó chạy càng nhiều bước. Hình hình tròn bên trên cho thấy phân bố màu từ rất ít cho đến nhiều, mỗi màu tượng trưng cho các từ theo thứ tự, "What", "Time", "is", "it" và "?", từ đó cho thấy "What" và "Time" có rất ít thông tin và gân như là không tồn tại, khi mà mạng RNN xử lý đến bước cuối cùng. Short-term memory và vanishing gradient xảy ra là do bản chất của back-propagation. Back-prop là một thuật toán dùng để train và tối ưu mạng neural. Trước tiên cùng xem lại sự tác động của back-pro đến deep feed-forward network.

Trainning neural network có 3 bước. Đầu tiên là đưa dữ liệu vào, foward và thực hiện dự đoán. Sau đó so sanh kết quả vừa dự đoán được với giá trị thật(ground-truth) bằng cách sử dụng hàm mất mát(loss function). Đầu ra của loss function là một giá trị sai số dùng để đánh giá xem mạng neural có hoat động tốt hay không, giá trị sai số này càng cao thì độ chính xác càng kém và ngược lại. Cuối cùng back-prop dùng nó để tính toán gradient cho mỗi node trong neural-network. 

<center>
  <img src="/img/bp/2018-06-18-RNN/backpropa.gif" alt="None">
 </center>
 
 Giá trị của gradient dùng để điều chỉnh các trọng số(weights) trong mạng neural giúp cho nó có thể học được từ dữ liệu. Gradient càng lớn nghĩa là phải điều chỉnh các trọng số càng nhiều và ngược lại. Khi thực hiện back-prop mỗi node trong layer tính toán gradient của nó dựa trên tác động của gradient trong layer trước đố. Vậy nên nếu sự điều chỉnh trọng số trong layer trước đó nhỏ thì các bước sau sẽ càng nhỏ hơn.
 
 Điều đó dẫn đến giá trị gradient bé dần theo cấp số mũ mỗi khi thực hiện một bước back-prop. Các layer trước đó sẽ không học được gì khi mà trọng số của nó gần như không được điều chỉnh vì giá trị gradient của nó cực kì bé. Và đó là vanishing gradient.

<center>
  <img src="/img/bp/2018-06-18-RNN/backpropashrink.gif" alt="None">
 <br>
 <em>Gradients shrink as it back-propagates down</em>
 </center>
 
 Để train RNN, chúng ta sử dụng một ứng dụng của back-propagation gọi là back-propagation through time. Khi thực hiện back-prop through time cứ qua mỗi bước(time step) giá trị của gradient giảm(shrink) dần theo cấp số mũ, mỗi time step của RNN được coi như là một layer trong neural network.
 
<center>
  <img src="/img/bp/2018-06-18-RNN/backpropatime.gif" alt="None">
 <br>
 <em>Gradients shrink as it back-propagates through time</em>
 </center>
 
 Gradient dùng để điều chỉnh trọng số(weights) của mạng neural để cho nó có thể học được từ dữ liệu. Gradient bé sẽ dẫn đến việc các layer trước đó sẽ không thể học được.
 
 RNN sẽ không thể học được từ những thông tin quá xa trước đó do vanishing gradient. Và khả năng cao là các từ "What" và "time" sẽ không được cân nhắc khi dự đoán mục đích của người dùng. RNN sẽ sử dụng "is it?" để dự đoán nhiều hơn là "What time", điều đó khiến việc dự đoán thực sự rất mơ hồ khó khăn ngay cả với con người. Vậy từ việc không thể học được từ những thông tin trước đó dẫn đến việc RNN có một vấn đề là short-term memory.
 
 
 Để giải quyết vấn đề này mạng RNN đã được upgrade, LSTM(long short-term memory) và  GRU(gated recurrent units), RNN sẽ sử dụng một trong hai mô hình LSTM hoặc GRU.
