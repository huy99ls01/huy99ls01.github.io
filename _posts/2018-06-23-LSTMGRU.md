---
title: "LSTM và GRU"
subtitle: "LSTM và GRU hoạt động như nào?"
layout: post
tags: [LSTM, GRU, Long Short Term Memory, Gated Recurrent Unit]
---

## The Problem, Short-term Memory

Như chúng ta đã biết từ phần một, RNN gặp phải một vấn đề trong việc xử lý một chuỗi dữ liệu dài, đó là short-term memory.
Và sẽ gặp khó khăn trong việc vận chuyển thông tin từ những bước đầu tiên cho đến bước cuối trong quá trình xử lý dữ liệu, 
nếu chúng ta muốn dự đoán một đoạn văn bản thì rnn có lẽ sẽ bỏ rơi những thông tin quan trọng ngay từ đầu văn bản.
Trong quá trình rnn thực hiện back-propagation sẽ gặp phải vanishing gradient. Trong đó gradient là giá trị dùng để điều chỉnh trọng số(weight) trong mạng neural networks. Vanishing gradient xảy ra khi giá trị gradient trở nên rất nhỏ sau mỗi bước back-prop và nếu giá trị của gradient rất nhỏ thì trọng số của mạng neural sẽ gần như không thay đổi và điều đó có nghĩa là mạng neural sẽ không thể học(learn) và tối ưu được.

 <center>
  <img src="/img/bp/2018-06-18-LSTMGRU/gradientupdate.png" alt="None">
  <br>
  <i>Gradient Update Rule</i>
 </center>
 
 
