---
title: "LSTM và GRU"
subtitle: "LSTM và GRU hoạt động như nào?"
layout: post
tags: [LSTM, GRU, Long Short Term Memory, Gated Recurrent Unit]
---

## The Problem, Short-term Memory

Như chúng ta đã biết từ phần một, RNN gặp phải một vấn đề trong việc xử lý một chuỗi dữ liệu dài, đó là short-term memory.
Và sẽ gặp khó khăn trong việc vận chuyển thông tin từ những bước đầu tiên cho đến bước cuối trong quá trình xử lý dữ liệu, 
nếu chúng ta muốn dự đoán một đoạn văn bản thì rnn có lẽ sẽ bỏ rơi những thông tin quan trọng ngay từ đầu văn bản.
Trong quá trình rnn thực hiện back-propagation sẽ gặp phải vanishing gradient. Trong đó gradient là giá trị dùng để điều chỉnh trọng số(weight) trong mạng neural networks. Vanishing gradient xảy ra khi giá trị gradient trở nên rất nhỏ sau mỗi bước back-prop và nếu giá trị của gradient rất nhỏ thì trọng số của mạng neural sẽ gần như không thay đổi và điều đó có nghĩa là mạng neural sẽ không thể học(learn) và tối ưu được.

 <center>
  <img src="/img/bp/2018-06-23-LSTMGRU/gradientupdate.png" alt="None">
  <br>
  <i>Gradient Update Rule</i>
 </center>
 
 Thường thì những layer đầu tiên trong mạng recurrent sẽ không được điều chỉnh hay học được do gradient rất nhỏ. Và vì những layer đầu tiên không học được nên dẫn đến việc rnn có thể sẽ bỏ quên những thông tin ban đầu khi mà phải xử lý chuỗi dữ liệu dài. 

## LSTM’s and GRU’s as a solution

LSTM và GRU được tạo ra để giải quyết vấn đề về short-term memory. Và đều dùng chung một cơ chế được gọi là cổng(gate) để điều chỉnh luồng thông tin.

 <center>
  <img src="/img/bp/2018-06-23-LSTMGRU/lstmgrucompare.png" alt="None">
 </center>
 
 Các cổng này được học từ dữ liệu và từ đó quyết định xem thông tin quan trọng nào trong chuỗi dữ liệu được giữ lại hoặc bỏ đi. Bằng cách làm như vậy nó có thể đưa những thông tin liên quan trong chuỗi dữ liệu đến bước cuối của quá trình xử lý chuỗi dữ liệu và thực hiện dự đoán. Hầu hết những bài toán sử dụng mạng recurrent đạt được kết quả rất tốt đều dùng LSTM hoặc GRU. LSTM và GRU còn được dùng trong nhận dạng giọng nói(speech recognition), tổng hợp giọng nói(speech synthesis) và sinh văn bản(text generation), và còn có thể dùng để sinh ra các chú thích(generate captions) cho video.

## Intuition

Ok, đầu tiên thử làm một thí nghiệm trước nhé. Giả sử như bờ dô đang đọc review của khách hàng về sản phẩm để quyết định xem có nên mua sản phẩm đó hay không. Điều đầu tiên bờ dô làm là đọc review để xem có tay nào cho biết quan điểm của hắn về sản phẩm này có ok hay không.

 <center>
  <img src="/img/bp/2018-06-23-LSTMGRU/review.png" alt="None">
 </center>

Trong khi bờ dô đang đọc review thì tiềm thức bộ não của bờ dô chỉ nhớ những từ khóa quan trọng, như là từ "amazing", "perfectly balanced breakfast". Bờ dô lúc nãy sẽ không ce lắm đến mấy từ như "this", "gave", "all", "should" ... Nếu hôm sau bạn của bờ dô có hỏi tay review đã nói gì thì có thể bờ dô sẽ không nhớ hết 100% những gì hắn nói, và có thể chỉ nhớ những đoạn quan trọng như "will definitely be buying again". Đối với một số người thì những chữ còn lại sẽ mờ nhạt trong trí nhớ.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/reviewfade.gif" alt="None">
 </center>

LSTM và GRU sẽ được học để nhận biết những thông tin liên quan nào được giữ lại giúp cho việc dự đoán tốt hơn, và sẽ không giữ lại những thông tin không liên quan, trong trường hợp này thì những từ tích cực để đánh giá sản phẩm là những thông tin có liên quan và đó là những từ mà bờ dô sẽ nhớ khi đọc review, nó giúp bờ dô có cái nhìn tốt hơn về việc đánh giá sản phẩm đó tốt hay không. 

## Review of Recurrent Neural Networks

Để hiểu được LSTM và GRU, trước tiên bờ dô hãy cùng xem lại cách hoạt động của recurrent neural networks. Đầu tiên mỗi từ sẽ được biến thành một vector để máy tính có thể đọc được. Sau đó xử lý các vector đó lần lượt từng cái một.


 <center>
  <img src="/img/bp/2018-06-23-LSTMGRU/process.gif" alt="None">
  <br>
  <i>Processing sequence one by one</i>
 </center>

Trong khi xử lý, nó chuyển tiếp thông tin từ hidden state trước đó đến bước tiếp theo của chuỗi. Hidden state đóng vai trò như bộ nhớ của mạng neural. Nó giữ thông tin của dữ liệu trước đó mà neural networks đã xử lý.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/passing.gif" alt="None">
  <br>
  <i>Passing hidden state to next time step</i>
 </center>

Hãy cùng nhìn vào một cell của RNN. Đầu tiên input và hidden state trước đó được gộp lại thành một vector. Vector này giờ đã có thông tin của input hiện tại và input trước đó. Vector này sẽ đi qua hàm tanh, và output mới trở thành một hidden state cũng như bộ nhớ của mạng neural.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/rnncell.gif" alt="None">
  <br>
  <i>RNN Cell</i>
 </center>

## Tanh activation

Hàm tanh activation giúp điều chỉnh giá trị khi đi qua mạng neural. Hàm tanh luôn đưa giá trị về khoảng giữa -1 và 1 

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/tanh.gif" alt="None">
  <br>
  <i>Tanh squishes values to be between -1 and 1</i>
 </center>

Khi vector đi qua mạng neural, nó trải qua rất nhiều biển đổi bởi các công thức toán học. Hãy tự tưởng tượng khi một giá trị liên tục được nhân với 3, bờ dô sẽ thấy nó trở nên cực kì lớn và dẫn đến việc các giá trị bé trở nên không còn ý nghĩa nữa.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/vectornotanh.gif" alt="None">
  <br>
  <i>vector transformations without tanh</i>
 </center>
 
 Hàm tanh này đảm bảo các giá trị sẽ luôn ở trong khoảng -1 và 1, điều này giúp cho việc điều chỉnh output của network. Bờ dô có thể thấy các giá trị giống nhau ở ảnh gif bên trên và giá trị của ảnh gif bên dưới được duy trì trong khoảng -1 đến 1 

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/vectortanh.gif" alt="None">
  <br>
  <i>vector transformations with tanh</i>
 </center>
 
 Vậy đó là RNN. Nó sử dụng rất ít các phép tính toán nhưng hoạt động rất hiệu quả trong những trường hợp nhất định(short sequences). RNN thực hiện ít công việc tính toán hơn các biến thể của nó là LSTM và GRU.
 
 ## LSTM
 
 LSTM có cơ chế điều khiển thông tin giống như rnn, nó xử lý dữ liệu và chuyển tiếp dữ liệu đến các node tiếp theo mỗi khi thực hiện propagate forward. Điều khác biệt giữa LSTM và RNN là ở các phép toán ở trong LSTM. 

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/lstmop.png" alt="None">
  <br>
  <i>LSTM Cell and It’s Operations</i>
 </center>

Và những phép toán này được dùng để LSTM quyết định những thông tin quan trọng nào cần giữ lại và thông tin nào cần bỏ đi.

## Core Concept

Khái niệm chính trong LSTM là các cổng và cell state. Cell state đóng vai trò như là một đường cao tốc vận chuyển thông tin tới tận cuối cùng của chuỗi, bờ dô có thể nghĩ nó như là một bộ nhớ của RNN. Về lý thuyết thì cell state vận chuyển những thông tin có tính liên quan đến tận cuối cùng trong quá trình xử lý chuỗi dữ liệu, qua đó làm giảm tác động của short-term memory. Mỗi khi qua từng bước xử lý hay đi qua một cell state, 
cell state sẽ thêm thông tin vào hoặc loại bỏ đi thông qua các cổng(gates). Các cổng sẽ quyết định những thông tin nào sẽ ở trong cell state đó và có thể học để giữ lại những thông tin có tính liên quan và loại bỏ những thông tin không liên quan trong quá trình training.

## Sigmoid

Các cổng sẽ chứa hàm sigmoid và hàm sigmoid cũng giống như hàm tanh nhưng thay vì đưa các giá trị về khoảng -1 đến 1, hàm sigmoid đưa các giá trị về khoảng 0 đến 1. Hàm này giúp cho việc cập nhập hoặc bỏ qua những thông tin, vì bất kỳ số nào nhân với 0 đều bằng 0, dẫn đến việc giá trị đó sẽ biến mất hoặc bị "lãng quên", và bất kỳ con số nào nhân với 1 đều bằng chính nó điều đó có nghĩa là giá trị đó sẽ được "giữ lại". Như đã nói thì những gate này có thể học để nhận biết thông tin nào quan trọng sẽ được giữ lại và ngược lại.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/sigmoid.gif" alt="None">
  <br>
  <i>Sigmoid squishes values to be between 0 and 1</i>
 </center>
 
 LSTM có 3 cổng khác nhau để điều chỉnh thông thông tin trong các cell state. Đó là các cổng forget gate, input gate, output gate.
 
 ## Forget gate

Đầu tiên là cổng forget, cổng này sẽ quyết định thông tin nào sẽ được giữ lại hoặc loại bỏ. Thông tin từ những hidden state trước đó và thông tin từ input hiện tại sẽ được đưa qua hàm sigmoid, giá trị đầu ra của nó sẽ rơi vào khoảng 0 đến 1. Càng gần 0 nghĩa là sẽ bị loại bỏ, và càng gần 1 sẽ được giữ lại.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/forgetop.gif" alt="None">
  <br>
  <i>Forget gate operations</i>
 </center>

## Input Gate

Để update cell state, ta có input gate. Đầu tiên, ta đưa thông tin từ hidden state trước đó và thông tin từ input hiện tại vào hàm sigmoid, nó sẽ quyết định giá trị nào sẽ được cập nhập bằng cách biến đổi các giá trị về khoảng 0 đến 1, 0 có nghĩa là thông tin không quan trọng, 1 có nghĩa là thông tin quan trọng. Trong khi đó thông tin từ hidden state trước đó và thông tin từ input hiện tại cũng sẽ được đưa vào hàm tanh để đưa các giá trị về khoảng -1 đến 1 giúp cho việc điều chỉnh mạng network. Sau đó ta nhân output của hàm tanh với output của hàm sigmoid, output của hàm sigmoid sẽ quyết định thông tin nào của hàm tanh là quan trọng và sẽ được giữ lại.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/inputop.gif" alt="None">
  <br>
  <i>Input gate operations</i>
 </center>
 
 ## Cell State

Và bây giờ chúng ta đã có đủ thông tin để tính toán cho cell state. Đầu tiên ta sẽ dùng phép toán pointwise multiplied của cell state trước đó với forget gate. Phép tính vừa rồi có khả năng sẽ làm một số giá trị ở cell state sẽ bị giảm đi nếu nó nhân với giá trị gần bằng 0. Sau đó chúng ta sẽ pointwise addition với output của cổng input, cuối cùng ta có cell state mới sẽ được cập nhập giá trị mới mà mạng neural thấy có tính tương đồng. 

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/cellstate.gif" alt="None">
  <br>
  <i>Calculating cell state</i>
 </center>

## Output Gate

Cuối cùng ta có output gate. Output gate là cổng sẽ quyết định hidden state tiếp theo sẽ như nào. Đầu tiên ta đưa thông tin từ hidden state trước đó và thông tin từ input hiện tại vào trong hàm sigmoid, sau đó chúng ta đưa cell state mới mà chúng ta vừa tính được vào hàm tanh. Từ đó ta nhân output của hàm sigmoid với output của hàm tanh để quyết định thông tin của hidden state. Đầu ra của output gate là hidden state mới. Cell state mới và hidden state mới sẽ được đưa qua bước tiếp theo để thực hiện tính toán.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/outputgate.gif" alt="None">
  <br>
  <i>output gate operations</i>
 </center>
 
 Tóm lại forget gate quyết định thông tin liên quan nào sẽ được giữ lại từ bước trước. Input gate sẽ quyết định những thông tin liên quan nào sẽ được thêm vào từ input hiện tại. Output gate sẽ quyết định những thông tin mới của hidden state mới.

## GRU

Hiện tại ta đã biết cách hoạt động của LSTM, hãy cùng nhìn lướt qua GRU. GRU là phiên bản mới hơn của recurrent networks và nó khá giống LSTM. GRU bỏ đi cell state và sử dụng hidden state để vận chuyển thông tin và nó chỉ có 2 cổng, reset gate và update gate.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/grucell.png" alt="None">
  <br>
  <i>GRU cell and it’s gates</i>
 </center>
 
 ## Update Gate

Update gate hoạt động giống như forget gate và input gate, nó quyết định thông tin mới sẽ được thêm vào và thông tin sẽ bị loại bỏ.

## Reset Gate

Reset gate được dùng để quyết định bao nhiêu thông tin từ bước trước sẽ bị lãng quên.

Và đó là GRU và nó có ít phép toán hơn LSTM nhưng sẽ nhanh hơn LSTM trong quá trình training, và cũng không có kết quả cho thấy GRU tốt hơn hay LSTM tốt hơn. Thông thường chúng ta sẽ thử cả 2 trường hợp để xem cái nào tốt hơn cho bài toán của chúng ta.

## So That’s it

Tóm lại là RNN rất tốt trong việc xử lý chuỗi dữ liệu để thực hiện dự đoán nhưng lại gặp phải vấn đề khi phải xử lý chuỗi dữ liệu dài, đó là short-term memory, LSTM và GRU được tạo ra để giải quyết short-term memory bằng cách sử dụng cơ chế được gọi là cổng. Các cổng chỉ là mạng neural dùng để điều chỉnh luồng thông tin thông qua chuỗi dữ liệu. LSTM và GRU còn được dùng trong các ứng dụng khác như speech recognition, speech synthesis, natural language understanding, image captioning, ...
