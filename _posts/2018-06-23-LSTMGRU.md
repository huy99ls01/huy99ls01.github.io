---
title: "LSTM và GRU"
subtitle: "LSTM và GRU hoạt động như nào?"
layout: post
tags: [LSTM, GRU, Long Short Term Memory, Gated Recurrent Unit]
---

## The Problem, Short-term Memory

Như chúng ta đã biết từ phần một, RNN gặp phải một vấn đề trong việc xử lý một chuỗi dữ liệu dài, đó là short-term memory.
Và sẽ gặp khó khăn trong việc vận chuyển thông tin từ những bước đầu tiên cho đến bước cuối trong quá trình xử lý dữ liệu, 
nếu chúng ta muốn dự đoán một đoạn văn bản thì rnn có lẽ sẽ bỏ rơi những thông tin quan trọng ngay từ đầu văn bản.
Trong quá trình rnn thực hiện back-propagation sẽ gặp phải vanishing gradient. Trong đó gradient là giá trị dùng để điều chỉnh trọng số(weight) trong mạng neural networks. Vanishing gradient xảy ra khi giá trị gradient trở nên rất nhỏ sau mỗi bước back-prop và nếu giá trị của gradient rất nhỏ thì trọng số của mạng neural sẽ gần như không thay đổi và điều đó có nghĩa là mạng neural sẽ không thể học(learn) và tối ưu được.

 <center>
  <img src="/img/bp/2018-06-23-LSTMGRU/gradientupdate.png" alt="None">
  <br>
  <i>Gradient Update Rule</i>
 </center>
 
 Thường thì những layer đầu tiên trong mạng recurrent sẽ không được điều chỉnh hay học được do gradient rất nhỏ. Và vì những layer đầu tiên không học được nên dẫn đến việc rnn có thể sẽ bỏ quên những thông tin ban đầu khi mà phải xử lý chuỗi dữ liệu dài. 

## LSTM’s and GRU’s as a solution

LSTM và GRU được tạo ra để giải quyết vấn đề về short-term memory. Và đều dùng chung một cơ chế được gọi là cổng(gate) để điều chỉnh luồng thông tin.

 <center>
  <img src="/img/bp/2018-06-23-LSTMGRU/lstmgrucompare.png" alt="None">
 </center>
 
 Các cổng này được học từ dữ liệu và từ đó quyết định xem thông tin quan trọng nào trong chuỗi dữ liệu được giữ lại hoặc bỏ đi. Bằng cách làm như vậy nó có thể đưa những thông tin liên quan trong chuỗi dữ liệu đến bước cuối của quá trình xử lý chuỗi dữ liệu và thực hiện dự đoán. Hầu hết những bài toán sử dụng mạng recurrent đạt được kết quả rất tốt đều dùng LSTM hoặc GRU. LSTM và GRU còn được dùng trong nhận dạng giọng nói(speech recognition), tổng hợp giọng nói(speech synthesis) và sinh văn bản(text generation), và còn có thể dùng để sinh ra các chú thích(generate captions) cho video.

## Intuition

Ok, đầu tiên thử làm một thí nghiệm trước nhé. Giả sử như bờ dô đang đọc review của khách hàng về sản phẩm để quyết định xem có nên mua sản phẩm đó hay không. Điều đầu tiên bờ dô làm là đọc review để xem có tay nào cho biết quan điểm của hắn về sản phẩm này có ok hay không.

 <center>
  <img src="/img/bp/2018-06-23-LSTMGRU/review.png" alt="None">
 </center>

Trong khi bờ dô đang đọc review thì tiềm thức bộ não của bờ dô chỉ nhớ những từ khóa quan trọng, như là từ "amazing", "perfectly balanced breakfast". Bờ dô lúc nãy sẽ không ce lắm đến mấy từ như "this", "gave", "all", "should" ... Nếu hôm sau bạn của bờ dô có hỏi tay review đã nói gì thì có thể bờ dô sẽ không nhớ hết 100% những gì hắn nói, và có thể chỉ nhớ những đoạn quan trọng như "will definitely be buying again". Đối với một số người thì những chữ còn lại sẽ mờ nhạt trong trí nhớ.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/reviewfade.gif" alt="None">
 </center>

LSTM và GRU sẽ được học để nhận biết những thông tin liên quan nào được giữ lại giúp cho việc dự đoán tốt hơn, và sẽ không giữ lại những thông tin không liên quan, trong trường hợp này thì những từ tích cực để đánh giá sản phẩm là những thông tin có liên quan và đó là những từ mà bờ dô sẽ nhớ khi đọc review, nó giúp bờ dô có cái nhìn tốt hơn về việc đánh giá sản phẩm đó tốt hay không. 

## Review of Recurrent Neural Networks

Để hiểu được LSTM và GRU, trước tiên bờ dô hãy cùng xem lại cách hoạt động của recurrent neural networks. Đầu tiên mỗi từ sẽ được biến thành một vector để máy tính có thể đọc được. Sau đó xử lý các vector đó lần lượt từng cái một.


 <center>
  <img src="/img/bp/2018-06-23-LSTMGRU/process.gif" alt="None">
  <br>
  <i>Processing sequence one by one</i>
 </center>

Trong khi xử lý, nó chuyển tiếp thông tin từ hidden state trước đó đến bước tiếp theo của chuỗi. Hidden state đóng vai trò như bộ nhớ của mạng neural. Nó giữ thông tin của dữ liệu trước đó mà neural networks đã xử lý.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/passing.gif" alt="None">
  <br>
  <i>Passing hidden state to next time step</i>
 </center>

Hãy cùng nhìn vào một cell của RNN. Đầu tiên input và hidden state trước đó được gộp lại thành một vector. Vector này giờ đã có thông tin của input hiện tại và input trước đó. Vector này sẽ đi qua hàm tanh, và output mới trở thành một hidden state cũng như bộ nhớ của mạng neural.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/rnncell.gif" alt="None">
  <br>
  <i>RNN Cell</i>
 </center>

## Tanh activation

Hàm tanh activation giúp điều chỉnh giá trị khi đi qua mạng neural. Hàm tanh luôn đưa giá trị về khoảng giữa -1 và 1 

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/tanh.gif" alt="None">
  <br>
  <i>Tanh squishes values to be between -1 and 1</i>
 </center>

Khi vector đi qua mạng neural, nó trải qua rất nhiều biển đổi bởi các công thức toán học. Hãy tự tưởng tượng khi một giá trị liên tục được nhân với 3, bờ dô sẽ thấy nó trở nên cực kì lớn và dẫn đến việc các giá trị bé trở nên không còn ý nghĩa nữa.

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/vectornotanh.gif" alt="None">
  <br>
  <i>vector transformations without tanh</i>
 </center>
 
 Hàm tanh này đảm bảo các giá trị sẽ luôn ở trong khoảng -1 và 1, điều này giúp cho việc điều chỉnh output của network. Bờ dô có thể thấy các giá trị giống nhau ở ảnh gif bên trên và giá trị của ảnh gif bên dưới được duy trì trong khoảng -1 đến 1 

<center>
  <img src="/img/bp/2018-06-23-LSTMGRU/vectortanh.gif" alt="None">
  <br>
  <i>vector transformations with tanh</i>
 </center>
 
 Vậy đó là RNN. Nó sử dụng rất ít các phép tính toán nhưng hoạt động rất hiệu quả trong những trường hợp nhất định(short sequences). RNN thực hiện ít công việc tính toán hơn các biến thể của nó là LSTM và GRU.

